import os
import json
import time
import re
from typing import Any, Dict, Generator, List, Optional, Tuple

import requests


# -----------------------------
# helpers
# -----------------------------
def env(name: str, default: str = "") -> str:
    v = os.getenv(name, default)
    return "" if v is None else str(v)


def as_int(v: str, default: int) -> int:
    try:
        return int(v)
    except Exception:
        return default


def as_bool(v: str, default: bool = False) -> bool:
    if v is None:
        return default
    return str(v).strip().lower() in ("1", "true", "yes", "y", "on")


def now_ts() -> int:
    return int(time.time())


def truncate(s: str, max_chars: int) -> str:
    if max_chars <= 0:
        return s
    if len(s) <= max_chars:
        return s
    return s[: max_chars - 1] + "‚Ä¶"


def pick_user_prompt(messages: List[Dict[str, Any]]) -> str:
    for m in reversed(messages or []):
        if m.get("role") == "user" and isinstance(m.get("content"), str):
            return m["content"].strip()
    if messages:
        c = messages[-1].get("content")
        if isinstance(c, str):
            return c.strip()
    return ""


def is_title_generation(body: Dict[str, Any]) -> bool:
    md = body.get("metadata") or {}
    if isinstance(md, dict) and md.get("task") == "title_generation":
        return True

    messages = body.get("messages") or []
    if isinstance(messages, list) and messages:
        last = messages[-1]
        content = last.get("content")
        if isinstance(content, str) and "Create a concise, 3-5 word title" in content:
            return True
    return False


def make_title(prompt: str) -> str:
    p = (prompt or "").strip()
    p = re.sub(r"\s+", " ", p)

    # –µ—Å–ª–∏ —ç—Ç–æ "Prompt: ...."
    m = re.search(r"Prompt:\s*(.*)$", p, flags=re.IGNORECASE)
    if m:
        p = m.group(1).strip()

    # —É–±–µ—Ä—ë–º –º—É—Å–æ—Ä–Ω—ã–µ —Ö–≤–æ—Å—Ç—ã
    p = p.strip().strip('"').strip("'")
    if not p:
        return "–ù–æ–≤—ã–π —á–∞—Ç"

    # –ø–µ—Ä–≤—ã–µ 3‚Äì6 —Å–ª–æ–≤
    words = [w for w in re.split(r"\s+", p) if w]
    title = " ".join(words[:6])
    title = title.replace('"', "").replace("'", "").strip()
    return title or "–ù–æ–≤—ã–π —á–∞—Ç"


def openai_response_text(model_id: str, content: str) -> Dict[str, Any]:
    t = now_ts()
    return {
        "id": f"chatcmpl-{t}",
        "object": "chat.completion",
        "created": t,
        "model": model_id,
        "choices": [
            {"index": 0, "message": {"role": "assistant", "content": content}, "finish_reason": "stop"}
        ],
    }


def stream_as_openai_chunks(model_id: str, full_text: str, chunk_size: int = 80) -> Generator[Dict[str, Any], None, None]:
    created = now_ts()
    # first chunk: role
    yield {
        "id": f"chatcmpl-{created}",
        "object": "chat.completion.chunk",
        "created": created,
        "model": model_id,
        "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}],
    }

    i = 0
    while i < len(full_text):
        part = full_text[i : i + chunk_size]
        i += chunk_size
        yield {
            "id": f"chatcmpl-{created}",
            "object": "chat.completion.chunk",
            "created": created,
            "model": model_id,
            "choices": [{"index": 0, "delta": {"content": part}, "finish_reason": None}],
        }

    # final chunk
    yield {
        "id": f"chatcmpl-{created}",
        "object": "chat.completion.chunk",
        "created": created,
        "model": model_id,
        "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
    }


# -----------------------------
# Qdrant + Ollama clients
# -----------------------------
def ollama_embeddings(ollama_url: str, model: str, text: str) -> List[float]:
    url = ollama_url.rstrip("/") + "/api/embeddings"
    payload = {"model": model, "prompt": text}
    r = requests.post(url, json=payload, timeout=180)
    r.raise_for_status()
    data = r.json()
    emb = data.get("embedding")
    if not isinstance(emb, list):
        raise RuntimeError(f"Unexpected embeddings response: {data}")
    return emb


def qdrant_search(qdrant_url: str, collection: str, vector: List[float], limit: int) -> List[Dict[str, Any]]:
    url = qdrant_url.rstrip("/") + f"/collections/{collection}/points/search"
    payload = {
        "vector": vector,
        "limit": limit,
        "with_payload": True,
        "with_vectors": False,
    }
    r = requests.post(url, json=payload, timeout=60)
    r.raise_for_status()
    data = r.json()
    return data.get("result", []) or []


def ollama_chat(ollama_url: str, model: str, messages: List[Dict[str, str]]) -> str:
    url = ollama_url.rstrip("/") + "/api/chat"
    payload = {"model": model, "messages": messages, "stream": False}
    r = requests.post(url, json=payload, timeout=900)
    r.raise_for_status()
    data = r.json()
    msg = (data.get("message") or {}).get("content")
    if not isinstance(msg, str):
        raise RuntimeError(f"Unexpected chat response: {data}")
    return msg


# -----------------------------
# payload parsing + pretty output
# -----------------------------
def payload_text(payload: Dict[str, Any]) -> str:
    # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–µ—Ä–ø–∏–º–æ –∫ —Ä–∞–∑–Ω—ã–º –∏–Ω–∂–µ—Å—Ç–µ—Ä–∞–º
    for k in ("text", "content", "chunk", "page_text", "body"):
        v = payload.get(k)
        if isinstance(v, str) and v.strip():
            return v.strip()
    return ""


def payload_source(payload: Dict[str, Any]) -> str:
    for k in ("source", "file", "filename", "document", "title", "path", "url"):
        v = payload.get(k)
        if isinstance(v, str) and v.strip():
            return v.strip()
    return "unknown"


def payload_page(payload: Dict[str, Any]) -> Optional[Any]:
    for k in ("page", "page_number", "pageno"):
        v = payload.get(k)
        if v is not None:
            return v
    return None


def build_context_and_cards(points: List[Dict[str, Any]], max_context_chars: int) -> Tuple[str, str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º:
      - context –¥–ª—è LLM (–ø–ª–æ—Ç–Ω—ã–π)
      - markdown –∫–∞—Ä—Ç–æ—á–∫–∏ "–ù–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"
    """
    ctx_parts: List[str] = []
    cards: List[str] = []

    for idx, p in enumerate(points, start=1):
        payload = p.get("payload") or {}
        if not isinstance(payload, dict):
            payload = {}

        txt = payload_text(payload)
        src = payload_source(payload)
        page = payload_page(payload)

        score = p.get("score")
        score_s = ""
        if isinstance(score, (int, float)):
            score_s = f" (score={score:.4f})"

        if not txt:
            continue

        # –∫–∞—Ä—Ç–æ—á–∫–∞: –∫–æ—Ä–æ—Ç–∫–∏–π —Å–Ω–∏–ø–ø–µ—Ç
        snippet = re.sub(r"\s+", " ", txt).strip()
        snippet = truncate(snippet, 420)

        where = src
        if page is not None:
            where = f"{src} ¬∑ —Å—Ç—Ä. {page}"

        cards.append(f"**{idx}. {where}**{score_s}\n\n> {snippet}\n")

        # –∫–æ–Ω—Ç–µ–∫—Å—Ç: —á—É—Ç—å –±–æ–ª—å—à–µ
        ctx_parts.append(f"[{idx}] {where}\n{txt.strip()}")

    context = "\n\n---\n\n".join(ctx_parts)
    context = truncate(context, max_context_chars)

    cards_md = ""
    if cards:
        cards_md = "## –ù–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏\n\n" + "\n\n".join(cards) + "\n"
    return context, cards_md


# -----------------------------
# Pipeline
# -----------------------------
class Pipeline:
    """
    Open WebUI Pipelines:
      - class Pipeline
      - method pipe(self, body: dict, user: dict | None)
    """

    def __init__(self) -> None:
        self.id = "rag-pipeline"
        self.name = "rag-pipeline"

        self.debug = as_bool(env("DEBUG_RAG", "0"), False)

        self.qdrant_url = env("QDRANT_URL", "http://qdrant.qdrant.svc.cluster.local:6333")
        self.qdrant_collection = env("QDRANT_COLLECTION", "docs_pdf")

        self.ollama_url = env("OLLAMA_URL", "http://ollama.ollama.svc.cluster.local:11434")
        self.embed_model = env("EMBED_MODEL", "nomic-embed-text:latest")
        self.llm_model = env("LLM_MODEL", "qwen3:1.7b")

        self.top_k = as_int(env("TOP_K", "5"), 5)
        self.max_context_chars = as_int(env("MAX_CONTEXT_CHARS", "8000"), 8000)

        # —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –∫–∞—à–∏, –µ—Å–ª–∏ Open WebUI –¥—ë—Ä–≥–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø–æ–¥—Ä—è–¥
        self._cache: Dict[str, Tuple[float, str]] = {}  # key -> (ts, full_answer)
        self._cache_ttl_sec = 60.0

    def _log(self, *args: Any) -> None:
        if self.debug:
            print("DEBUG_RAG:", *args, flush=True)

    def _cache_get(self, key: str) -> Optional[str]:
        v = self._cache.get(key)
        if not v:
            return None
        ts, ans = v
        if time.time() - ts > self._cache_ttl_sec:
            self._cache.pop(key, None)
            return None
        return ans

    def _cache_set(self, key: str, ans: str) -> None:
        self._cache[key] = (time.time(), ans)

    def pipe(self, body: Dict[str, Any], user: Optional[Dict[str, Any]] = None) -> Any:
        stream = bool(body.get("stream", True))
        messages = body.get("messages") or []
        if not isinstance(messages, list):
            messages = []

        # 1) –ó–∞–≥–æ–ª–æ–≤–æ–∫ —á–∞—Ç–∞ ‚Äî –Ω–µ RAG, –Ω–µ Ollama, –Ω–µ Qdrant
        if is_title_generation(body):
            prompt = pick_user_prompt(messages)
            title = make_title(prompt)
            return openai_response_text(self.id, title)

        # 2) –ù–æ—Ä–º–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        prompt = pick_user_prompt(messages)
        if not prompt:
            return openai_response_text(self.id, "–ù–∞–ø–∏—à–∏ –∑–∞–ø—Ä–æ—Å üôÇ")

        # key –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ (UI –º–æ–∂–µ—Ç –¥–µ—Ä–Ω—É—Ç—å 2-5 —Ä–∞–∑ –ø–æ–¥—Ä—è–¥)
        chat_id = body.get("chat_id") or ((body.get("metadata") or {}) if isinstance(body.get("metadata"), dict) else {}).get("chat_id")
        cache_key = f"{chat_id or 'nochat'}::{prompt}"

        cached = self._cache_get(cache_key)
        if cached:
            if stream:
                return stream_as_openai_chunks(self.id, cached)
            return openai_response_text(self.id, cached)

        # 2.1) Embeddings
        try:
            qvec = ollama_embeddings(self.ollama_url, self.embed_model, prompt)
        except Exception as e:
            msg = f"–ù–µ —Å–º–æ–≥ —Å–¥–µ–ª–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ Ollama ({self.embed_model}): {e}"
            if stream:
                return stream_as_openai_chunks(self.id, msg)
            return openai_response_text(self.id, msg)

        # 2.2) Qdrant search
        try:
            points = qdrant_search(self.qdrant_url, self.qdrant_collection, qvec, self.top_k)
        except Exception as e:
            msg = f"–ù–µ —Å–º–æ–≥ –∏—Å–∫–∞—Ç—å –≤ Qdrant ({self.qdrant_collection}): {e}"
            if stream:
                return stream_as_openai_chunks(self.id, msg)
            return openai_response_text(self.id, msg)

        context, cards_md = build_context_and_cards(points, self.max_context_chars)

        # –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî —á–µ—Å—Ç–Ω–æ –≥–æ–≤–æ—Ä–∏–º –∏ –≤—Å—ë
        if not context.strip():
            answer = (
                f"{cards_md}\n"
                "## –û—Ç–≤–µ—Ç\n\n"
                "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞—à—ë–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É. "
                "–ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä: `CI/CD GitLab Runner cache S3`, `ArgoCD sync hooks`, `Ingress TLS default cert`)."
            ).strip()
            self._cache_set(cache_key, answer)
            if stream:
                return stream_as_openai_chunks(self.id, answer)
            return openai_response_text(self.id, answer)

        # 2.3) LLM –æ—Ç–≤–µ—Ç (—Å—Ç—Ä–æ–≥–æ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É)
        system = (
            "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏.\n"
            "–û—Ç–≤–µ—á–∞–π –ø–æ-—Ä—É—Å—Å–∫–∏.\n"
            "–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏, —á—Ç–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\n"
            "–ü–∏—à–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–æ: –∫—Ä–∞—Ç–∫–æ, –∑–∞—Ç–µ–º —à–∞–≥–∏/–∫–æ–º–∞–Ω–¥—ã.\n"
            "–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã.\n"
        )

        user_msg = (
            f"–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n{prompt}\n\n"
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã):\n{context}\n\n"
            "–°—Ñ–æ—Ä–º–∏—Ä—É–π –æ—Ç–≤–µ—Ç. –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã/–ø—É—Ç–∏/–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ‚Äî –ø—Ä–∏–≤–µ–¥–∏ –∏—Ö."
        )

        try:
            llm_text = ollama_chat(
                self.ollama_url,
                self.llm_model,
                [{"role": "system", "content": system}, {"role": "user", "content": user_msg}],
            ).strip()
        except Exception as e:
            msg = f"–ù–µ —Å–º–æ–≥ –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Ollama ({self.llm_model}): {e}"
            if stream:
                return stream_as_openai_chunks(self.id, msg)
            return openai_response_text(self.id, msg)

        answer = (f"{cards_md}\n## –û—Ç–≤–µ—Ç\n\n{llm_text}").strip()
        self._cache_set(cache_key, answer)

        # –í–∞–∂–Ω–æ: —á—Ç–æ–±—ã OpenWebUI –Ω–µ –ø–ª–æ–¥–∏–ª –¥–≤–∞ –≤—ã–∑–æ–≤–∞ Ollama ‚Äî –º—ã —Å—Ç—Ä–∏–º–∏–º —É–∂–µ –≥–æ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        if stream:
            return stream_as_openai_chunks(self.id, answer)
        return openai_response_text(self.id, answer)
